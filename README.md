# CS324 Deep Learning Assignments

This repository contains the implementations of three major assignments for the **CS324: Deep Learning** course. The projects cover the implementation of fundamental neural network architectures‚Äîranging from simple Perceptrons to GANs‚Äîimplemented both **from scratch (using NumPy)** and using **PyTorch**.

---

## üìö Project Overview

### **Assignment 1: The Perceptron & NumPy MLP**
**Objective:** Understanding the theoretical foundations of neural networks by building them from scratch.

- **Part I: The Perceptron:**  
  Implementation of a single-layer perceptron to classify data generated from two Gaussian distributions.

- **Part II: Multi-Layer Perceptron (MLP):**
  - Built a flexible N-layer MLP using NumPy.
  - Implemented forward propagation (Affine transformations + ReLU/Softmax activations) and backward propagation (Cross Entropy Loss).
  - Tested on the `make_moons` dataset.

- **Part III: Optimization:**  
  Implemented Stochastic Gradient Descent (SGD) and compared its performance against Batch Gradient Descent.

---

### **Assignment 2: PyTorch Framework, CNNs, and RNNs**
**Objective:** Transitioning to deep learning frameworks and exploring vision and sequence models.

- **Part I: PyTorch MLP:**  
  Re-implemented the MLP architecture from Assignment 1 using PyTorch to compare accuracy against the NumPy version.

- **Part II: Convolutional Neural Networks (CNN):**
  - Implemented a CNN (a reduced version of VGG) to classify images from the **CIFAR10** dataset.
  - Utilized the Adam optimizer and mini-batch gradient descent.

- **Part III: Recurrent Neural Networks (RNN):**
  - Implemented a raw RNN (without `torch.nn.RNN`) to predict the last digit of a palindrome.
  - Analyzed the limitations of RNN memory by plotting accuracy against increasing palindrome lengths.

---

### **Assignment 3: LSTMs and Generative Adversarial Networks (GANs)**
**Objective:** Addressing vanishing gradients and exploring generative modeling.

- **Part I: Long Short-Term Memory (LSTM):**
  - Implemented an LSTM cell from scratch (using gate equations for input, forget, output, and modulation).
  - Applied the model to the palindrome prediction task to demonstrate improved performance over the vanilla RNN on longer sequences.

- **Part II: Generative Adversarial Networks (GAN):**
  - Implemented a MinMax game between a Generator and a Discriminator.
  - Trained on the **MNIST** dataset to generate handwritten digits from noise.
  - Performed latent space interpolation to visualize the transition between different digits.

---

## üõ†Ô∏è Technologies & Libraries

- **Python**
- **NumPy** (Linear algebra, manual backpropagation)
- **PyTorch** (Automatic differentiation, neural modules)
- **Torchvision** (Datasets: CIFAR10, MNIST)
- **Scikit-Learn** (Data generation: `make_moons`)
- **Matplotlib** (Visualization of loss curves and generated images)
- **Jupyter Notebooks** (Experimentation and reporting)

---

## üöÄ Installation & Usage

1. **Clone the repository:**
    ```bash
    git clone https://github.com/yourusername/CS324-Deep-Learning.git
    cd CS324-Deep-Learning
    ```

2. **Install dependencies:**
    ```bash
    pip install numpy torch torchvision scikit-learn matplotlib jupyter
    ```

3. **Navigate to the specific assignment folder (e.g., `Assignment_1`) and launch Jupyter:**
    ```bash
    jupyter notebook
    ```

---

## üìä Results & Analysis

Each assignment folder contains a report (or Jupyter Notebook) detailing:

- **Loss & Accuracy Curves:** Visualization of training vs. testing performance.
- **Ablation Studies:** Analysis of batch sizes and learning rates.
- **Generative Samples:** Images generated by the GAN at different training stages.

---

## üìú Credits

Course materials and assignment specifications provided by **Jianguo Zhang** for **CS324: Deep Learning**.
